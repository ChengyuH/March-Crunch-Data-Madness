{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d4bc6be",
   "metadata": {},
   "source": [
    "### Packages \n",
    "\n",
    "这里是之后用到的库 \n",
    "\n",
    "#### Pandas\n",
    "#### Numpy\n",
    "#### Matplotlib \n",
    "#### Math\n",
    "#### Sklearn\n",
    "#### Warning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8864faf",
   "metadata": {},
   "source": [
    "Statquest: 兄弟们有时间大家可以看看statquest这个channel 对几乎所有的统计和machine learning的概念解释的特别通俗易懂 非常有帮助\n",
    "对以后找工作和面试帮助应该非常大 \n",
    "\n",
    "https://www.youtube.com/@statquest/playlists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13873b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn \n",
    "import numpy as np \n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "\n",
    "import sklearn\n",
    "import warnings\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5eb936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tourney = pd.read_csv(\"NCAA_Tourney_2002_2022.csv\").query(\"season!=2019\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a795d2ba",
   "metadata": {},
   "source": [
    "### Mix Labels \n",
    "\n",
    "#### For the dataset provided, team1 have won all of the games, which is a vital flaw if we put the dataset into the model. Therefore, it's necessary for us to mix the labels, which contains both 0 and 1. \n",
    "\n",
    "#### Now, we try to random select rows from the dataset, and switch the position of team_1 and team_2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d653c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1313)\n",
    "\n",
    "remove_n = 650\n",
    "random_indices = np.random.choice(df_tourney.index, remove_n, replace=False)\n",
    "\n",
    "df1 = df_tourney.drop(random_indices)\n",
    "df2 = df_tourney.iloc[random_indices, :] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371934d4",
   "metadata": {},
   "source": [
    "#### team_1 -> team_2, team_1 -> team_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71e24efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns = df2.columns.str.replace(\"team1\",\"team3\",regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12c6fb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns = df2.columns.str.replace(\"team2\",\"team1\",regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d13e8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns = df2.columns.str.replace(\"team3\",\"team2\",regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dac26b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      team2_id  team2_score  team1_id  team1_score WLoc  num_ot  \\\n",
      "1208      1393           57      1395           52    N       0   \n",
      "34        1181           78      1168           61    N       0   \n",
      "1113      1291           67      1309           66    N       0   \n",
      "786       1243           70      1379           64    N       0   \n",
      "951       1276           79      1400           65    N       0   \n",
      "\n",
      "     team2_position team1_position  team2_seed  team1_seed  ...  team2_adjoe  \\\n",
      "1208           X11b            X06          11           6  ...     103.0000   \n",
      "34              X02            X15           2          15  ...     119.3570   \n",
      "1113           W16a           W16b          16          16  ...      98.9643   \n",
      "786             W08            W09           8           9  ...     108.6262   \n",
      "951             Y02            Y07           2           7  ...     122.2637   \n",
      "\n",
      "      team2_de team2_adjde team1_tempo team1_adjtempo  team1_oe team1_adjoe  \\\n",
      "1208   94.1000     98.3000     70.3000        68.7000  120.6000    116.0000   \n",
      "34     98.2346     95.6444     66.5151        66.0235  103.8070    104.6470   \n",
      "1113   99.3830    103.1910     68.5799        66.0122  101.9040    100.8620   \n",
      "786    93.6136     91.8746     64.4099        64.9346  107.1150    109.2343   \n",
      "951   103.0238    101.6228     69.4218        68.1663  106.3702    109.4989   \n",
      "\n",
      "      team1_de team1_adjde         game_id  \n",
      "1208  101.7000    106.4000  2018-1393-1395  \n",
      "34     99.3304    102.5460  2022-1181-1168  \n",
      "1113   99.4903    101.3720  2017-1291-1309  \n",
      "786    99.9955     99.2821  2012-1243-1379  \n",
      "951   100.5983     96.3269  2014-1276-1400  \n",
      "\n",
      "[5 rows x 104 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df2.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d219ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.concat([df2, df1]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d5c10d",
   "metadata": {},
   "source": [
    "### Data Munipulation\n",
    "\n",
    "#### Create New Features (建立新的特征)\n",
    "\n",
    "之后我们可以在这里面创建一些新的variable \n",
    "\n",
    "Meeting的时候我们可以找一些其他的数据构建新的variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee062d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed Difference \n",
    "df[\"sead_diff\"] = df[\"team1_seed\"] - df[\"team2_seed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79f84e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regions of each team \n",
    "df['team1_region'] = df[\"team1_position\"].str[:1] \n",
    "df['team2_region'] = df[\"team2_position\"].str[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b0bf199",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['team1_position', 'team2_position'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4ad44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected Win Rates\n",
    "df['exp_win1'] = (df['team1_adjoe']**11.5)/ ((df['team1_adjde']**11.5)+(df['team1_adjoe']**11.5))\n",
    "df['exp_win2'] = (df['team2_adjoe']**11.5)/ ((df['team2_adjde']**11.5)+(df['team2_adjoe']**11.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b44de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the distance between two teams  \n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "\n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0\n",
    "\n",
    "    lat1 = radians(lat1)\n",
    "    lon1 = radians(lon1)\n",
    "    lat2 = radians(lat2)\n",
    "    lon2 = radians(lon2)\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6224834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dist1'] = df.apply(lambda row: distance(row['host_lat'], row['host_long'], row['team1_lat'], row['team1_long']), axis=1)\n",
    "df['dist2'] = df.apply(lambda row: distance(row['host_lat'], row['host_long'], row['team2_lat'], row['team2_long']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "110bd415",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diff_dist'] = df['dist1'] - df['dist2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8da31cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['team1_win'] = (df['team1_score']>df['team2_score']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29798817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    650\n",
       "1    596\n",
       "Name: team1_win, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we can tell that labels have been successfully mixed \n",
    "df['team1_win'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ddc92d",
   "metadata": {},
   "source": [
    "#### Drop IDs Variables \n",
    "\n",
    "We need to drop IDs, since it doesn't make any sense if we include in our dataset. \n",
    "\n",
    "ID在modeling中是没有用的，如果把它们放到模型中会被当成numerical variable影响预测 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97a3c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns={\"team1_id\",\"team2_id\",\"game_id\",\"team2_teamname\",\"team1_teamname\",\"slot\",\"team1_score\",\"team2_score\",\"team2_coach_id\",\"team1_coach_id\"},inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5db307",
   "metadata": {},
   "source": [
    "### Feature Selection (特征选择)\n",
    "\n",
    "Since there are so many variables in the dataset provided. For a glimpse, we can tell a few issues. \n",
    "\n",
    "1. multicollinearity(多重共线性): 两个自变量直接高度相关, 会导致预测结果overfitting和coefficients不准确 (违反了linear regression的假设) \n",
    "\n",
    "2. Missing Value (缺失值): 有些自变量有很多缺失值，如果我们自己Impute的话会出现不准确影响预测 \n",
    "\n",
    "3. Low variance (低方差): 自变量和因变量之间的方差低说明自变量和因变量直接的线性关系不明显，自变量没用\n",
    "\n",
    "#### Missing Value Ration (缺失值)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5c14238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# Keep the ratio of null values \n",
    "a = df.isnull().sum()/len(df)*100\n",
    "# keep the feature names \n",
    "variables = df.columns\n",
    "variable = [ ]\n",
    "\n",
    "# Filter features if the ratio is above the threshold\n",
    "for i in range(len(variables)):\n",
    "    if a[i]<=20:   #setting the threshold as 10%\n",
    "        variable.append(variables[i])\n",
    "\n",
    "# Check how many variables were filtered \n",
    "print(len(variables) - len(variable))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09db6df",
   "metadata": {},
   "source": [
    "#### Filter (过滤法) \n",
    "   ##### Low Variance Filter  \n",
    "   \n",
    "   We usually think that features with low variance carry very little information. We calculate the variance of all features, and we delete features under the threshold we set\n",
    "   \n",
    "Variance Inflation Factor (VIF) 的中文翻译是方差膨胀因子。它是一种评估多重共线性的统计学指标，用于衡量自变量对因变量的影响程度。VIF 值越大，说明自变量之间的共线性越强，因此通常需要减少自变量数量以提高模型的稳定性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d11f08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = df.select_dtypes(include=np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20c587c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = numeric.var()\n",
    "\n",
    "numeric = numeric.columns\n",
    "numeric_var = []\n",
    "\n",
    "for i in range(0,len(var)):\n",
    "    if var[i]>=10:   # 将阈值设置为10％\n",
    "        numeric_var.append(numeric[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "877f63b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_var.remove(\"team1_win\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eefce59",
   "metadata": {},
   "source": [
    "##### Variance Inflation Factor (因子膨胀分析） \n",
    "\n",
    " Remove multicollinearities\n",
    " \n",
    "To remove multicollinearities, we can do two things. We can create new features or remove them from our data.\n",
    "\n",
    "Removing features is not recommended at first. The reason is that there’s a possibility of information loss because we remove that feature. Therefore, we will generate new features first.\n",
    "\n",
    "From those features, we can generate the new one. The new feature will contain the difference value between those pairs. After we create those features, we can safely remove them from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7033909f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      feature       VIF\n",
      "64  diff_dist  4.862231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\2345Downloads\\lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:193: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "\n",
    "x = df[numeric_var].dropna()\n",
    "vif['feature'] = x.columns\n",
    "vif['VIF'] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]\n",
    "\n",
    "numeric_var2 = vif.loc[vif['VIF'] < 10]\n",
    "\n",
    "# Display the VIF Result, we can tell that basically all features suffer from multicollinearity\n",
    "print(numeric_var2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d03883",
   "metadata": {},
   "source": [
    "#### L1 Regularization (Lasso正则化) \n",
    "\n",
    "We can tell that there are some pairs of features. \n",
    "\n",
    "As we know, L1 Regularization would be a great technique to select features to avoid multicollinearity.\n",
    "\n",
    "Therefore, for the next part, we will try Regularization to find the most appropriate features.\n",
    "\n",
    "\n",
    "L1正则化，也称为Lasso正则化，是机器学习和统计中使用的一种技术，它在损失函数中加入一个惩罚项，以限制模型的复杂度。 L1正则化通过限制系数的绝对值，来达到对结果的约束。它与L2正则化（Ridge正则化）的不同在于，L1正则化会产生稀疏解，即有些系数会变为0。\n",
    "\n",
    "Ex: LogisticRegressionCV( penalty='l1')  \n",
    "\n",
    "有时间看一下这俩视频: \n",
    "\n",
    "https://www.youtube.com/watch?v=Q81RR3yKn30 \n",
    "\n",
    "https://www.youtube.com/watch?v=NGf0voTMlcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2379bf0",
   "metadata": {},
   "source": [
    "##### Construct Train and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68655774",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"team1_win\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eff67484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['team1_seed', 'strongseed', 'season', 'host_lat', 'host_long', 'team2_lat', 'team2_long', 'team1_lat', 'team1_long', 'team2_pt_school_ncaa', 'team2_pt_overall_ncaa', 'team2_pt_school_s16', 'team2_pt_overall_s16', 'team2_pt_school_ff', 'team2_pt_career_school_losses', 'team2_pt_career_overall_wins', 'team2_pt_career_overall_losses', 'team2_pt_team_season_wins', 'team2_pt_team_season_losses', 'team2_pt_coach_season_wins', 'team2_pt_coach_season_losses', 'team1_pt_school_ncaa', 'team1_pt_overall_ncaa', 'team1_pt_school_s16', 'team1_pt_overall_s16', 'team1_pt_school_ff', 'team1_pt_career_school_losses', 'team1_pt_career_overall_wins', 'team1_pt_career_overall_losses', 'team1_pt_team_season_wins', 'team1_pt_team_season_losses', 'team1_pt_coach_season_wins', 'team1_pt_coach_season_losses', 'team2_ap_preseason', 'team2_coaches_before_final', 'team2_coaches_preseason', 'team1_ap_final', 'team1_ap_preseason', 'team1_coaches_before_final', 'team1_coaches_preseason', 'team2_fg2pct', 'team2_blockpct', 'team2_oppf3grate', 'team2_arate', 'team2_opparate', 'team2_stlrate', 'team1_blockpct', 'team1_oppf3grate', 'team1_arate', 'team1_opparate', 'team1_stlrate', 'team2_adjtempo', 'team2_adjoe', 'team2_de', 'team2_adjde', 'team1_tempo', 'team1_adjtempo', 'team1_oe', 'team1_adjoe', 'team1_de', 'team1_adjde', 'sead_diff', 'exp_win1', 'dist2', 'diff_dist']\n"
     ]
    }
   ],
   "source": [
    "print(numeric_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63f8cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_var = []\n",
    "remove = []\n",
    "\n",
    "# The dependent variable team1_win have been removed from this loop \n",
    "for column in numeric:\n",
    "    if column not in numeric_var:\n",
    "            remove.append(column)\n",
    "\n",
    "df.drop(columns=remove, inplace=True)\n",
    "\n",
    "for column in df.columns:\n",
    "    if column not in numeric_var:s\n",
    "        categorical_var.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9904df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c49348b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fca5fc",
   "metadata": {},
   "source": [
    "##### Buliding Pipeline for Logistic Regression CV\n",
    "\n",
    "\n",
    "这个视频可以直观理解 Logistic Regression CV\n",
    "https://www.youtube.com/watch?v=fSytzGwwBVw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3120285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant')),\n",
    "    ('scaler',StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "        ('num', numerical_transformer, numeric_var),\n",
    "        ('cat', categorical_transformer, categorical_var)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3cd6c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K - fold Cross Validation\n",
    "kfolds = 5\n",
    "\n",
    "# The range of penalty levels\n",
    "min_alpha = 0.1\n",
    "max_alpha = 100\n",
    "n_candidates = 1000\n",
    "C_list = list(1/np.linspace(min_alpha, max_alpha, num=n_candidates))\n",
    "\n",
    "# Model Pipeline \n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                            ('model', LogisticRegressionCV(Cs=C_list, cv=kfolds, penalty='l1',scoring='neg_log_loss',solver='liblinear', max_iter=2000, random_state=1, n_jobs=-1))\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d931c2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='constant')),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['team1_seed', 'strongseed',\n",
       "                                                   'season', 'host_lat',\n",
       "                                                   'host_long', 'team2_lat',\n",
       "                                                   'team2_long', 'team1_lat',\n",
       "                                                   'team1_long',\n",
       "                                                   'team2_pt_school_ncaa',\n",
       "                                                   'team2_pt_overall_ncaa',\n",
       "                                                   'team2_pt_school_s16',\n",
       "                                                   'team2_p...\n",
       "                                          0.588235294117647, 0.5555555555555555,\n",
       "                                          0.5263157894736842, 0.5,\n",
       "                                          0.47619047619047616,\n",
       "                                          0.45454545454545453,\n",
       "                                          0.4347826086956521,\n",
       "                                          0.41666666666666663,\n",
       "                                          0.3999999999999999,\n",
       "                                          0.3846153846153846,\n",
       "                                          0.37037037037037035,\n",
       "                                          0.3571428571428571,\n",
       "                                          0.3448275862068965,\n",
       "                                          0.33333333333333326, ...],\n",
       "                                      cv=5, max_iter=2000, n_jobs=-1,\n",
       "                                      penalty='l1', random_state=1,\n",
       "                                      scoring='neg_log_loss',\n",
       "                                      solver='liblinear'))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f401e6a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5881241183637567\n"
     ]
    }
   ],
   "source": [
    "preds = pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(metrics.log_loss(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b711bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_names(column_transformer):\n",
    "    \"\"\"Get feature names from all transformers.\n",
    "    Returns\n",
    "    -------\n",
    "    feature_names : list of strings\n",
    "        Names of the features produced by transform.\n",
    "    \"\"\"\n",
    "    # Remove the internal helper function\n",
    "    #check_is_fitted(column_transformer)\n",
    "    \n",
    "    # Turn loopkup into function for better handling with pipeline later\n",
    "    def get_names(trans):\n",
    "        # >> Original get_feature_names() method\n",
    "        if trans == 'drop' or (\n",
    "                hasattr(column, '__len__') and not len(column)):\n",
    "            return []\n",
    "        if trans == 'passthrough':\n",
    "            if hasattr(column_transformer, '_df_columns'):\n",
    "                if ((not isinstance(column, slice))\n",
    "                        and all(isinstance(col, str) for col in column)):\n",
    "                    return column\n",
    "                else:\n",
    "                    return column_transformer._df_columns[column]\n",
    "            else:\n",
    "                indices = np.arange(column_transformer._n_features)\n",
    "                return ['x%d' % i for i in indices[column]]\n",
    "        if not hasattr(trans, 'get_feature_names'):\n",
    "        # >>> Change: Return input column names if no method avaiable\n",
    "            # Turn error into a warning\n",
    "            warnings.warn(\"Transformer %s (type %s) does not \"\n",
    "                                 \"provide get_feature_names. \"\n",
    "                                 \"Will return input column names if available\"\n",
    "                                 % (str(name), type(trans).__name__))\n",
    "            # For transformers without a get_features_names method, use the input\n",
    "            # names to the column transformer\n",
    "            if column is None:\n",
    "                return []\n",
    "            else:\n",
    "                return [name + \"__\" + f for f in column]\n",
    "\n",
    "        return [name + \"__\" + f for f in trans.get_feature_names()]\n",
    "    \n",
    "    ### Start of processing\n",
    "    feature_names = []\n",
    "    \n",
    "    # Allow transformers to be pipelines. Pipeline steps are named differently, so preprocessing is needed\n",
    "    if type(column_transformer) == sklearn.pipeline.Pipeline:\n",
    "        l_transformers = [(name, trans, None, None) for step, name, trans in column_transformer._iter()]\n",
    "    else:\n",
    "        # For column transformers, follow the original method\n",
    "        l_transformers = list(column_transformer._iter(fitted=True))\n",
    "    \n",
    "    \n",
    "    for name, trans, column, _ in l_transformers: \n",
    "        if type(trans) == sklearn.pipeline.Pipeline:\n",
    "            # Recursive call on pipeline\n",
    "            _names = get_feature_names(trans)\n",
    "            # if pipeline has no transformer that returns names\n",
    "            if len(_names)==0:\n",
    "                _names = [name + \"__\" + f for f in column]\n",
    "            feature_names.extend(_names)\n",
    "        else:\n",
    "            feature_names.extend(get_names(trans))\n",
    "    \n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40993a80",
   "metadata": {},
   "source": [
    "##### Return a dataframe contains all feature names and coefficients \n",
    "\n",
    "For features returned coeficients of 0, we abandon these features for future model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "839afd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19493\\AppData\\Local\\Temp/ipykernel_38564/1809191144.py:30: UserWarning: Transformer imputer (type SimpleImputer) does not provide get_feature_names. Will return input column names if available\n",
      "  warnings.warn(\"Transformer %s (type %s) does not \"\n",
      "C:\\Users\\19493\\AppData\\Local\\Temp/ipykernel_38564/1809191144.py:30: UserWarning: Transformer scaler (type StandardScaler) does not provide get_feature_names. Will return input column names if available\n",
      "  warnings.warn(\"Transformer %s (type %s) does not \"\n"
     ]
    }
   ],
   "source": [
    "df_feature = pd.DataFrame(pipeline.named_steps['model'].coef_.flatten(), index=get_feature_names(preprocessor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "17f2608a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.000000    132\n",
       "-0.198781      1\n",
       "-0.041571      1\n",
       "-0.007171      1\n",
       " 0.032801      1\n",
       " 0.089930      1\n",
       "-0.032672      1\n",
       "-0.084550      1\n",
       "-0.236445      1\n",
       " 0.003284      1\n",
       " 0.140342      1\n",
       "-0.203006      1\n",
       "-0.620468      1\n",
       "-0.003772      1\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "40942aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = df_feature[df_feature[0] != 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "25cecad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          0\n",
      "num__team1_seed                   -0.198781\n",
      "num__team2_pt_school_ncaa         -0.041571\n",
      "num__team2_pt_team_season_wins    -0.007171\n",
      "num__team2_pt_coach_season_losses  0.032801\n",
      "num__team1_pt_overall_s16          0.089930\n",
      "num__team2_ap_preseason           -0.032672\n",
      "num__team2_blockpct               -0.084550\n",
      "num__team2_adjoe                  -0.236445\n",
      "num__team2_adjde                   0.003284\n",
      "num__team1_oe                      0.140342\n",
      "num__team1_adjde                  -0.203006\n",
      "num__sead_diff                    -0.620468\n",
      "num__diff_dist                    -0.003772\n"
     ]
    }
   ],
   "source": [
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e478a",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "\n",
    "As we can see from select_feature dataframe, there are 13 features returned not 0 from Logistic Regression with l1 penalty. The logisticRegressionCV method gives us a comprehensive picture of what types of feature are most informative. \n",
    "\n",
    "The followings are features we selected with description.\n",
    "\n",
    "(We did some minor changes, for example team1_oe and team1_adjoe are actually the same thing. The model returns the coef for team1_oe and abandon team1_adjoe ; however, for better understanding purpose, we choose team1_adjoe instead) \n",
    "\n",
    "Team Variables: \n",
    "\n",
    "1. sead_diff: The difference of seed between two schools. \n",
    "\n",
    "2. adjde: an estimate of the defense efficiency (points scored per 100 possessions) a team would have against the average D-I defense. \n",
    "\n",
    "3. adjoe: An estimate of the offensive efficiency (points scored per 100 possessions) a team would have against the average D-I defense.\n",
    "\n",
    "4. blockpct - Blocked shots divided by opponents 2 point field goal attempts. \n",
    "\n",
    "5. ap_preseason: The preseason AP Poll ranking of each team (top 25 only) \n",
    "\n",
    "6. ap_final: The final AP Poll ranking of each team (top 25 only)\n",
    "\n",
    "7. team_season_wins: Team’s number of wins in this season \n",
    "\n",
    "8. diff_dist: The distance between two schools \n",
    "\n",
    "Coach Variables: \n",
    "\n",
    "8. pt_overall_s16: Number of NCAA Sweet Sixteen appearances in entire career \n",
    "\n",
    "9. pt_coach_season_wins: Coach’s number of wins in this season\n",
    "\n",
    "10. pt_school_ncaa: Number of NCAA Tournament appearances at current school"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d400c6",
   "metadata": {},
   "source": [
    "有时间看一下这几个gradient boost \n",
    "\n",
    "GradientBoost: https://www.youtube.com/watch?v=3CC4N4z3GJc&t=826s \n",
    "\n",
    "Xgboost: https://www.youtube.com/watch?v=8b1JEDvenQU \n",
    "\n",
    "为了accuracy 我们要使用组合模型： \n",
    "\n",
    "模型组合的方法包括：\n",
    "\n",
    "Bagging（Bootstrapped Aggregating）：基于随机抽样的多个模型的简单平均，以减小单一模型的方差。\n",
    "\n",
    "https://www.youtube.com/watch?v=2Mg8QD0F1dQ\n",
    "\n",
    "Boosting：通过加强弱模型的权重以提高性能，最终通过加权平均以组合多个弱模型。\n",
    "\n",
    "Stacking：通过使用第二层模型对多个第一层模型的预测进行组合。\n",
    "\n",
    "https://www.youtube.com/watch?v=DCrcoh7cMHU\n",
    "\n",
    "Adaboost：通过加强正确预测样本的权重，以提高弱模型的性能。\n",
    "\n",
    "https://www.youtube.com/watch?v=LsK-xG1cLYA\n",
    "\n",
    "Random Forest：通过使用随机抽样的决策树构建多个模型，并以简单平均或投票结果组合多个模型。\n",
    "\n",
    "https://www.youtube.com/watch?v=dD7gvbfBiyA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.509px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
